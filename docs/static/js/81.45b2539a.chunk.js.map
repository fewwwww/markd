{"version":3,"sources":["../node_modules/@codemirror/legacy-modes/mode/shell.js"],"names":["words","define","style","dict","i","length","commonAtoms","commonKeywords","commonCommands","tokenBase","stream","state","eatSpace","delim","sol","ch","next","tokens","unshift","tokenString","tokenize","eat","skipToEnd","tokenDollar","eatWhile","match","heredoc","string","shift","test","eol","peek","cur","current","hasOwnProperty","quote","close","escaped","backUp","tokenStringStart","shell","startState","token","languageData","autocomplete","concat","closeBrackets","brackets","commentTokens","line"],"mappings":"qGAAA,iDAAIA,EAAQ,GAEZ,SAASC,EAAOC,EAAOC,GACrB,IAAK,IAAIC,EAAI,EAAGA,EAAID,EAAKE,OAAQD,IAC/BJ,EAAMG,EAAKC,IAAMF,EAKrB,IAAII,EAAc,CAAC,OAAQ,SACvBC,EAAiB,CAAC,KAAM,OAAQ,KAAM,OAAQ,OAAQ,QAAS,QAAS,MAAO,KAAM,OAAQ,KAAM,MAAO,MAAO,OAAQ,OAAQ,MAAO,QAAS,SAAU,YAC3JC,EAAiB,CAAC,KAAM,MAAO,OAAQ,OAAQ,MAAO,KAAM,KAAM,QAAS,QAAS,SAAU,QAAS,KAAM,OAAQ,MAAO,OAAQ,OAAQ,OAAQ,OAAQ,MAAO,MAAO,MAAO,OAAQ,KAAM,OAAQ,UAAW,KAAM,KAAM,OAAQ,QAAS,UAAW,KAAM,KAAM,KAAM,OAAQ,MAAO,OAAQ,KAAM,UAAW,KAAM,QAAS,MAAO,UAAW,KAAM,QAAS,QAAS,SAAU,OAAQ,QAAS,MAAO,QAAS,OAAQ,KAAM,OAAQ,MAAO,MAAO,SAAU,MAAO,QAAS,KAAM,MAAO,OAAQ,KAAM,OAAQ,MAAO,QAAS,MAAO,OAKphB,SAASC,EAAUC,EAAQC,GACzB,GAAID,EAAOE,WAAY,OAAO,KAC9B,IAqHoBC,EArHhBC,EAAMJ,EAAOI,MACbC,EAAKL,EAAOM,OAEhB,GAAW,OAAPD,EAEF,OADAL,EAAOM,OACA,KAGT,GAAW,MAAPD,GAAsB,MAAPA,GAAqB,MAAPA,EAE/B,OADAJ,EAAMM,OAAOC,QAAQC,EAAYJ,EAAW,MAAPA,EAAa,QAAU,WACrDK,EAASV,EAAQC,GAG1B,GAAW,MAAPI,EACF,OAAID,GAAOJ,EAAOW,IAAI,MACpBX,EAAOY,YACA,SAGTZ,EAAOY,YACA,WAGT,GAAW,MAAPP,EAEF,OADAJ,EAAMM,OAAOC,QAAQK,GACdH,EAASV,EAAQC,GAG1B,GAAW,MAAPI,GAAqB,MAAPA,EAChB,MAAO,WAGT,GAAW,MAAPA,EAGF,OAFAL,EAAOW,IAAI,KACXX,EAAOc,SAAS,MACT,YAGT,GAAU,KAANT,EAAW,CACb,GAAIL,EAAOe,MAAM,MAAO,MAAO,WAC/B,IAAIC,EAAUhB,EAAOe,MAAM,6BAE3B,GAAIC,EAEF,OADAf,EAAMM,OAAOC,SA0EGL,EA1EkBa,EAAQ,GA2EvC,SAAUhB,EAAQC,GAGvB,OAFID,EAAOI,OAASJ,EAAOiB,QAAUd,GAAOF,EAAMM,OAAOW,QACzDlB,EAAOY,YACA,oBA7EE,iBAIX,GAAI,KAAKO,KAAKd,KACZL,EAAOc,SAAS,MAEZd,EAAOoB,QAAU,KAAKD,KAAKnB,EAAOqB,SACpC,MAAO,SAIXrB,EAAOc,SAAS,SAChB,IAAIQ,EAAMtB,EAAOuB,UACjB,MAAsB,MAAlBvB,EAAOqB,QAAkB,MAAMF,KAAKG,GAAa,MAC9ChC,EAAMkC,eAAeF,GAAOhC,EAAMgC,GAAO,KAGlD,SAASb,EAAYgB,EAAOjC,GAC1B,IAAIkC,EAAiB,KAATD,EAAe,IAAe,KAATA,EAAe,IAAMA,EACtD,OAAO,SAAUzB,EAAQC,GAIvB,IAHA,IAAIK,EACAqB,GAAU,EAEmB,OAAzBrB,EAAON,EAAOM,SAAiB,CACrC,GAAIA,IAASoB,IAAUC,EAAS,CAC9B1B,EAAMM,OAAOW,QACb,MACK,GAAa,MAATZ,IAAiBqB,GAAqB,MAAVF,GAAiBzB,EAAOqB,QAAUK,EAAO,CAC9EC,GAAU,EACV3B,EAAO4B,OAAO,GACd3B,EAAMM,OAAOC,QAAQK,GACrB,MACK,IAAKc,GAAWF,IAAUC,GAASpB,IAASmB,EAEjD,OADAxB,EAAMM,OAAOC,QAAQC,EAAYgB,EAAOjC,IACjCkB,EAASV,EAAQC,GACnB,IAAK0B,GAAW,OAAOR,KAAKb,KAAU,OAAOa,KAAKM,GAAQ,CAC/DxB,EAAMM,OAAOC,QAAQqB,EAAiBvB,EAAM,WAC5CN,EAAO4B,OAAO,GACd,MAGFD,GAAWA,GAAoB,OAATrB,EAGxB,OAAOd,GAMX,SAASqC,EAAiBJ,EAAOjC,GAC/B,OAAO,SAAUQ,EAAQC,GAGvB,OAFAA,EAAMM,OAAO,GAAKE,EAAYgB,EAAOjC,GACrCQ,EAAOM,OACAI,EAASV,EAAQC,IAzG5BV,EAAO,OAAQK,GACfL,EAAO,UAAWM,GAClBN,EAAO,UAAWO,GA2GlB,IAAIe,EAAc,SAAqBb,EAAQC,GACzCA,EAAMM,OAAOZ,OAAS,GAAGK,EAAOW,IAAI,KACxC,IAAIN,EAAKL,EAAOM,OAEhB,MAAI,SAASa,KAAKd,IAChBJ,EAAMM,OAAO,GAAKE,EAAYJ,EAAU,KAANA,EAAY,QAAgB,KAANA,EAAY,MAAQ,UACrEK,EAASV,EAAQC,KAGrB,KAAKkB,KAAKd,IAAKL,EAAOc,SAAS,MACpCb,EAAMM,OAAOW,QACN,QAWT,SAASR,EAASV,EAAQC,GACxB,OAAQA,EAAMM,OAAO,IAAMR,GAAWC,EAAQC,GAIzC,IAAI6B,EAAQ,CACjBC,WAAY,WACV,MAAO,CACLxB,OAAQ,KAGZyB,MAAO,SAAehC,EAAQC,GAC5B,OAAOS,EAASV,EAAQC,IAE1BgC,aAAc,CACZC,aAActC,EAAYuC,OAAOtC,EAAgBC,GACjDsC,cAAe,CACbC,SAAU,CAAC,IAAK,IAAK,IAAK,IAAK,IAAK,MAEtCC,cAAe,CACbC,KAAM","file":"static/js/81.45b2539a.chunk.js","sourcesContent":["var words = {};\n\nfunction define(style, dict) {\n  for (var i = 0; i < dict.length; i++) {\n    words[dict[i]] = style;\n  }\n}\n\n;\nvar commonAtoms = [\"true\", \"false\"];\nvar commonKeywords = [\"if\", \"then\", \"do\", \"else\", \"elif\", \"while\", \"until\", \"for\", \"in\", \"esac\", \"fi\", \"fin\", \"fil\", \"done\", \"exit\", \"set\", \"unset\", \"export\", \"function\"];\nvar commonCommands = [\"ab\", \"awk\", \"bash\", \"beep\", \"cat\", \"cc\", \"cd\", \"chown\", \"chmod\", \"chroot\", \"clear\", \"cp\", \"curl\", \"cut\", \"diff\", \"echo\", \"find\", \"gawk\", \"gcc\", \"get\", \"git\", \"grep\", \"hg\", \"kill\", \"killall\", \"ln\", \"ls\", \"make\", \"mkdir\", \"openssl\", \"mv\", \"nc\", \"nl\", \"node\", \"npm\", \"ping\", \"ps\", \"restart\", \"rm\", \"rmdir\", \"sed\", \"service\", \"sh\", \"shopt\", \"shred\", \"source\", \"sort\", \"sleep\", \"ssh\", \"start\", \"stop\", \"su\", \"sudo\", \"svn\", \"tee\", \"telnet\", \"top\", \"touch\", \"vi\", \"vim\", \"wall\", \"wc\", \"wget\", \"who\", \"write\", \"yes\", \"zsh\"];\ndefine('atom', commonAtoms);\ndefine('keyword', commonKeywords);\ndefine('builtin', commonCommands);\n\nfunction tokenBase(stream, state) {\n  if (stream.eatSpace()) return null;\n  var sol = stream.sol();\n  var ch = stream.next();\n\n  if (ch === '\\\\') {\n    stream.next();\n    return null;\n  }\n\n  if (ch === '\\'' || ch === '\"' || ch === '`') {\n    state.tokens.unshift(tokenString(ch, ch === \"`\" ? \"quote\" : \"string\"));\n    return tokenize(stream, state);\n  }\n\n  if (ch === '#') {\n    if (sol && stream.eat('!')) {\n      stream.skipToEnd();\n      return 'meta'; // 'comment'?\n    }\n\n    stream.skipToEnd();\n    return 'comment';\n  }\n\n  if (ch === '$') {\n    state.tokens.unshift(tokenDollar);\n    return tokenize(stream, state);\n  }\n\n  if (ch === '+' || ch === '=') {\n    return 'operator';\n  }\n\n  if (ch === '-') {\n    stream.eat('-');\n    stream.eatWhile(/\\w/);\n    return 'attribute';\n  }\n\n  if (ch == \"<\") {\n    if (stream.match(\"<<\")) return \"operator\";\n    var heredoc = stream.match(/^<-?\\s*['\"]?([^'\"]*)['\"]?/);\n\n    if (heredoc) {\n      state.tokens.unshift(tokenHeredoc(heredoc[1]));\n      return 'string.special';\n    }\n  }\n\n  if (/\\d/.test(ch)) {\n    stream.eatWhile(/\\d/);\n\n    if (stream.eol() || !/\\w/.test(stream.peek())) {\n      return 'number';\n    }\n  }\n\n  stream.eatWhile(/[\\w-]/);\n  var cur = stream.current();\n  if (stream.peek() === '=' && /\\w+/.test(cur)) return 'def';\n  return words.hasOwnProperty(cur) ? words[cur] : null;\n}\n\nfunction tokenString(quote, style) {\n  var close = quote == \"(\" ? \")\" : quote == \"{\" ? \"}\" : quote;\n  return function (stream, state) {\n    var next,\n        escaped = false;\n\n    while ((next = stream.next()) != null) {\n      if (next === close && !escaped) {\n        state.tokens.shift();\n        break;\n      } else if (next === '$' && !escaped && quote !== \"'\" && stream.peek() != close) {\n        escaped = true;\n        stream.backUp(1);\n        state.tokens.unshift(tokenDollar);\n        break;\n      } else if (!escaped && quote !== close && next === quote) {\n        state.tokens.unshift(tokenString(quote, style));\n        return tokenize(stream, state);\n      } else if (!escaped && /['\"]/.test(next) && !/['\"]/.test(quote)) {\n        state.tokens.unshift(tokenStringStart(next, \"string\"));\n        stream.backUp(1);\n        break;\n      }\n\n      escaped = !escaped && next === '\\\\';\n    }\n\n    return style;\n  };\n}\n\n;\n\nfunction tokenStringStart(quote, style) {\n  return function (stream, state) {\n    state.tokens[0] = tokenString(quote, style);\n    stream.next();\n    return tokenize(stream, state);\n  };\n}\n\nvar tokenDollar = function tokenDollar(stream, state) {\n  if (state.tokens.length > 1) stream.eat('$');\n  var ch = stream.next();\n\n  if (/['\"({]/.test(ch)) {\n    state.tokens[0] = tokenString(ch, ch == \"(\" ? \"quote\" : ch == \"{\" ? \"def\" : \"string\");\n    return tokenize(stream, state);\n  }\n\n  if (!/\\d/.test(ch)) stream.eatWhile(/\\w/);\n  state.tokens.shift();\n  return 'def';\n};\n\nfunction tokenHeredoc(delim) {\n  return function (stream, state) {\n    if (stream.sol() && stream.string == delim) state.tokens.shift();\n    stream.skipToEnd();\n    return \"string.special\";\n  };\n}\n\nfunction tokenize(stream, state) {\n  return (state.tokens[0] || tokenBase)(stream, state);\n}\n\n;\nexport var shell = {\n  startState: function startState() {\n    return {\n      tokens: []\n    };\n  },\n  token: function token(stream, state) {\n    return tokenize(stream, state);\n  },\n  languageData: {\n    autocomplete: commonAtoms.concat(commonKeywords, commonCommands),\n    closeBrackets: {\n      brackets: [\"(\", \"[\", \"{\", \"'\", '\"', \"`\"]\n    },\n    commentTokens: {\n      line: \"#\"\n    }\n  }\n};"],"sourceRoot":""}